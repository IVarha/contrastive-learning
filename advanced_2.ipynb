{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-22T14:17:14.506357Z",
     "start_time": "2025-05-22T14:17:14.174554Z"
    }
   },
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"ayaroshevskiy/downsampled-imagenet-64x64\")\n",
    "print(\"Path to dataset files:\", path)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/igor.varha/miniconda3/envs/samp_hum_home/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.12)\n",
      "Path to dataset files: /Users/igor.varha/.cache/kagglehub/datasets/ayaroshevskiy/downsampled-imagenet-64x64/versions/1\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T14:18:44.946804Z",
     "start_time": "2025-05-22T14:18:44.944464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "train_path = Path(path)/'train_64x64'/'train_64x64'\n",
    "test_path = Path(path)/'valid_64x64/valid_64x64'\n",
    "train_dataset_p = \"tr_dataset.pt\"\n",
    "test_dataset_p = \"te_dataset.pt\""
   ],
   "id": "f40577fe944715a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T14:27:14.688342Z",
     "start_time": "2025-05-22T14:27:14.685971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from image_toolkit.data_processor import FragmentDataset\n",
    "from image_toolkit.clustering import evaluate_clustering_on_validation_p\n",
    "import pickle\n",
    "from torch_geometric.data import DataLoader\n",
    "\n"
   ],
   "id": "e9d7d3302b25de43",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T14:20:44.846832Z",
     "start_time": "2025-05-22T14:20:39.701429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "if Path(train_dataset_p).exists() and Path(test_dataset_p).exists():\n",
    "    train_dataset = pickle.load(open(train_dataset_p, \"rb\"))\n",
    "    test_dataset = pickle.load(open(test_dataset_p, \"rb\"))\n",
    "else:\n",
    "    test_dataset = FragmentDataset(test_path,limit=1000)\n",
    "\n",
    "    pickle.dump(test_dataset, open(test_dataset_p, \"wb\"))\n",
    "    train_dataset = FragmentDataset(train_path,limit=100000)\n",
    "    pickle.dump(train_dataset, open(train_dataset_p, \"wb\"))"
   ],
   "id": "b37c923b943e8d32",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T14:27:52.841504Z",
     "start_time": "2025-05-22T14:27:52.839252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "patch_size = 8\n",
    "n_patches = 64\n",
    "\n",
    "test_dataset.fragments_per_image = n_patches\n",
    "test_dataset.patch_size = patch_size\n",
    "\n",
    "train_dataset.fragments_per_image = n_patches\n",
    "train_dataset.patch_size = patch_size\n",
    "dataloader_test = DataLoader(test_dataset, batch_size=10, shuffle=False)\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=10, shuffle=True)"
   ],
   "id": "d94e2c3e0a19739",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/igor.varha/miniconda3/envs/samp_hum_home/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T14:23:32.938188Z",
     "start_time": "2025-05-22T14:23:32.870708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ],
   "id": "ddef249fd39ded16",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T14:24:04.839114Z",
     "start_time": "2025-05-22T14:24:03.520710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from image_toolkit.nets import TransformerPatchCluster\n",
    "\n",
    "model = TransformerPatchCluster(embed_dim=256,nhead=8,device=DEVICE,num_layers=7).to(DEVICE) #0.72\n",
    "model.load_weights(\"best_TTC_256_8_8_ARI90(100K)/best_model_epoch_78.pth\")\n",
    "# load best model"
   ],
   "id": "756806a7334082e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights loaded from best_TTC_256_8_8_ARI90(100K)/best_model_epoch_78.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/igor.varha/PycharmProjects/samp_hum_home/image_toolkit/nets.py:306: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path, map_location=self.device))\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T14:24:40.100016Z",
     "start_time": "2025-05-22T14:24:33.306464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# test on default test set\n",
    "ari,nmi,sil = evaluate_clustering_on_validation_p(dataloader_test,model,device=DEVICE)\n",
    "print(f\"ARI : {ari}, NMI: {nmi}, Silhouette: {sil}\")"
   ],
   "id": "3e4b8cecefb0ea71",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI : 0.13028539081906534, NMI: 0.23507703530713672, Silhouette: 0.2796148955821991\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T16:22:45.068604Z",
     "start_time": "2025-05-22T14:28:43.125498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#retraining model\n",
    "LR = 9e-5\n",
    "EPOCHS = 20\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)\n",
    "\n",
    "val_losses = model.train_model(dataloader_train,\n",
    "                  dataloader_test,\n",
    "                  optimizer,\n",
    "                  lr_scheduler,\n",
    "                  epochs=EPOCHS,\n",
    "                  device=DEVICE,\n",
    "                  temperature=0.33#,top_k=5\n",
    "                  )"
   ],
   "id": "47d0acd811514bf0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 10000/10000 [14:31<00:00, 11.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 5.2211\n",
      "Epoch [1/20], ARI: 0.7070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/igor.varha/miniconda3/envs/samp_hum_home/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at epoch 1 with ARI: 0.7070\n",
      "Current learning rate: [8.972587124713445e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 10000/10000 [13:14<00:00, 12.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Loss: 4.9236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/igor.varha/miniconda3/envs/samp_hum_home/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], ARI: 0.7550\n",
      "Model saved at epoch 2 with ARI: 0.7550\n",
      "Current learning rate: [8.968742855287973e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 10000/10000 [12:59<00:00, 12.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20], Loss: 4.8593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/igor.varha/miniconda3/envs/samp_hum_home/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20], ARI: 0.7833\n",
      "Model saved at epoch 3 with ARI: 0.7833\n",
      "Current learning rate: [8.966362025669304e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 10000/10000 [14:36<00:00, 11.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20], Loss: 4.8254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/igor.varha/miniconda3/envs/samp_hum_home/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20], ARI: 0.7961\n",
      "Model saved at epoch 4 with ARI: 0.7961\n",
      "Current learning rate: [8.965254899275425e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 10000/10000 [15:40<00:00, 10.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20], Loss: 4.7975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/igor.varha/miniconda3/envs/samp_hum_home/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20], ARI: 0.8048\n",
      "Model saved at epoch 5 with ARI: 0.8048\n",
      "Current learning rate: [8.964490663126493e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 10000/10000 [15:32<00:00, 10.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20], Loss: 4.7759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/igor.varha/miniconda3/envs/samp_hum_home/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20], ARI: 0.8130\n",
      "Model saved at epoch 6 with ARI: 0.8130\n",
      "Current learning rate: [8.963765708323359e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 10000/10000 [21:39<00:00,  7.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20], Loss: 4.7599\n",
      "Epoch [7/20], ARI: 0.8215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/igor.varha/miniconda3/envs/samp_hum_home/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at epoch 7 with ARI: 0.8215\n",
      "Current learning rate: [8.963000780280249e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20:  23%|██▎       | 2251/10000 [05:18<18:15,  7.08it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      4\u001B[39m optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n\u001B[32m      5\u001B[39m lr_scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=\u001B[32m1e-6\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m val_losses = model.train_model(dataloader_train,\n\u001B[32m      8\u001B[39m                   dataloader_test,\n\u001B[32m      9\u001B[39m                   optimizer,\n\u001B[32m     10\u001B[39m                   lr_scheduler,\n\u001B[32m     11\u001B[39m                   epochs=EPOCHS,\n\u001B[32m     12\u001B[39m                   device=DEVICE,\n\u001B[32m     13\u001B[39m                   temperature=\u001B[32m0.33\u001B[39m\u001B[38;5;66;03m#,top_k=5\u001B[39;00m\n\u001B[32m     14\u001B[39m                   )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/samp_hum_home/image_toolkit/nets.py:279\u001B[39m, in \u001B[36mtrain_model\u001B[39m\u001B[34m(self, train_loader, val_loader, optimizer, scheduler, device, epochs, temperature)\u001B[39m\n\u001B[32m    276\u001B[39m embeddings = \u001B[38;5;28mself\u001B[39m.forward(batch)\n\u001B[32m    277\u001B[39m \u001B[38;5;66;03m#print(embeddings.shape)\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m279\u001B[39m loss = supervised_nt_xent_loss(embeddings, labels, temperature=temperature)\n\u001B[32m    281\u001B[39m optimizer.zero_grad()\n\u001B[32m    282\u001B[39m loss.backward()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/samp_hum_home/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:137\u001B[39m, in \u001B[36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    135\u001B[39m opt = opt_ref()\n\u001B[32m    136\u001B[39m opt._opt_called = \u001B[38;5;28;01mTrue\u001B[39;00m  \u001B[38;5;66;03m# type: ignore[union-attr]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m137\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m func.\u001B[34m__get__\u001B[39m(opt, opt.\u001B[34m__class__\u001B[39m)(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/samp_hum_home/lib/python3.12/site-packages/torch/optim/optimizer.py:487\u001B[39m, in \u001B[36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    482\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    483\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m    484\u001B[39m                 \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    485\u001B[39m             )\n\u001B[32m--> \u001B[39m\u001B[32m487\u001B[39m out = func(*args, **kwargs)\n\u001B[32m    488\u001B[39m \u001B[38;5;28mself\u001B[39m._optimizer_step_code()\n\u001B[32m    490\u001B[39m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/samp_hum_home/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001B[39m, in \u001B[36m_use_grad_for_differentiable.<locals>._use_grad\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m     89\u001B[39m     torch.set_grad_enabled(\u001B[38;5;28mself\u001B[39m.defaults[\u001B[33m\"\u001B[39m\u001B[33mdifferentiable\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m     90\u001B[39m     torch._dynamo.graph_break()\n\u001B[32m---> \u001B[39m\u001B[32m91\u001B[39m     ret = func(\u001B[38;5;28mself\u001B[39m, *args, **kwargs)\n\u001B[32m     92\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m     93\u001B[39m     torch._dynamo.graph_break()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/samp_hum_home/lib/python3.12/site-packages/torch/optim/adamw.py:220\u001B[39m, in \u001B[36mAdamW.step\u001B[39m\u001B[34m(self, closure)\u001B[39m\n\u001B[32m    207\u001B[39m     beta1, beta2 = cast(Tuple[\u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;28mfloat\u001B[39m], group[\u001B[33m\"\u001B[39m\u001B[33mbetas\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m    209\u001B[39m     has_complex = \u001B[38;5;28mself\u001B[39m._init_group(\n\u001B[32m    210\u001B[39m         group,\n\u001B[32m    211\u001B[39m         params_with_grad,\n\u001B[32m   (...)\u001B[39m\u001B[32m    217\u001B[39m         state_steps,\n\u001B[32m    218\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m220\u001B[39m     adamw(\n\u001B[32m    221\u001B[39m         params_with_grad,\n\u001B[32m    222\u001B[39m         grads,\n\u001B[32m    223\u001B[39m         exp_avgs,\n\u001B[32m    224\u001B[39m         exp_avg_sqs,\n\u001B[32m    225\u001B[39m         max_exp_avg_sqs,\n\u001B[32m    226\u001B[39m         state_steps,\n\u001B[32m    227\u001B[39m         amsgrad=amsgrad,\n\u001B[32m    228\u001B[39m         beta1=beta1,\n\u001B[32m    229\u001B[39m         beta2=beta2,\n\u001B[32m    230\u001B[39m         lr=group[\u001B[33m\"\u001B[39m\u001B[33mlr\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m    231\u001B[39m         weight_decay=group[\u001B[33m\"\u001B[39m\u001B[33mweight_decay\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m    232\u001B[39m         eps=group[\u001B[33m\"\u001B[39m\u001B[33meps\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m    233\u001B[39m         maximize=group[\u001B[33m\"\u001B[39m\u001B[33mmaximize\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m    234\u001B[39m         foreach=group[\u001B[33m\"\u001B[39m\u001B[33mforeach\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m    235\u001B[39m         capturable=group[\u001B[33m\"\u001B[39m\u001B[33mcapturable\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m    236\u001B[39m         differentiable=group[\u001B[33m\"\u001B[39m\u001B[33mdifferentiable\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m    237\u001B[39m         fused=group[\u001B[33m\"\u001B[39m\u001B[33mfused\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m    238\u001B[39m         grad_scale=\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mgrad_scale\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[32m    239\u001B[39m         found_inf=\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mfound_inf\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[32m    240\u001B[39m         has_complex=has_complex,\n\u001B[32m    241\u001B[39m     )\n\u001B[32m    243\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/samp_hum_home/lib/python3.12/site-packages/torch/optim/optimizer.py:154\u001B[39m, in \u001B[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    152\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m disabled_func(*args, **kwargs)\n\u001B[32m    153\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m154\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m func(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/samp_hum_home/lib/python3.12/site-packages/torch/optim/adamw.py:782\u001B[39m, in \u001B[36madamw\u001B[39m\u001B[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[39m\n\u001B[32m    779\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    780\u001B[39m     func = _single_tensor_adamw\n\u001B[32m--> \u001B[39m\u001B[32m782\u001B[39m func(\n\u001B[32m    783\u001B[39m     params,\n\u001B[32m    784\u001B[39m     grads,\n\u001B[32m    785\u001B[39m     exp_avgs,\n\u001B[32m    786\u001B[39m     exp_avg_sqs,\n\u001B[32m    787\u001B[39m     max_exp_avg_sqs,\n\u001B[32m    788\u001B[39m     state_steps,\n\u001B[32m    789\u001B[39m     amsgrad=amsgrad,\n\u001B[32m    790\u001B[39m     beta1=beta1,\n\u001B[32m    791\u001B[39m     beta2=beta2,\n\u001B[32m    792\u001B[39m     lr=lr,\n\u001B[32m    793\u001B[39m     weight_decay=weight_decay,\n\u001B[32m    794\u001B[39m     eps=eps,\n\u001B[32m    795\u001B[39m     maximize=maximize,\n\u001B[32m    796\u001B[39m     capturable=capturable,\n\u001B[32m    797\u001B[39m     differentiable=differentiable,\n\u001B[32m    798\u001B[39m     grad_scale=grad_scale,\n\u001B[32m    799\u001B[39m     found_inf=found_inf,\n\u001B[32m    800\u001B[39m     has_complex=has_complex,\n\u001B[32m    801\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/samp_hum_home/lib/python3.12/site-packages/torch/optim/adamw.py:375\u001B[39m, in \u001B[36m_single_tensor_adamw\u001B[39m\u001B[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001B[39m\n\u001B[32m    372\u001B[39m param.mul_(\u001B[32m1\u001B[39m - lr * weight_decay)\n\u001B[32m    374\u001B[39m \u001B[38;5;66;03m# Decay the first and second moment running average coefficient\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m375\u001B[39m exp_avg.lerp_(grad, \u001B[32m1\u001B[39m - beta1)\n\u001B[32m    376\u001B[39m exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=\u001B[32m1\u001B[39m - beta2)\n\u001B[32m    378\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m capturable \u001B[38;5;129;01mor\u001B[39;00m differentiable:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Testing",
   "id": "db4cf0ab12c55282"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T16:23:14.925280Z",
     "start_time": "2025-05-22T16:23:07.876930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ari,nmi,sil = evaluate_clustering_on_validation_p(dataloader_test,model,device=DEVICE)\n",
    "print(f\"ARI : {ari}, NMI: {nmi}, Silhouette: {sil}\")"
   ],
   "id": "138286d547a861a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI : 0.8198463886422354, NMI: 0.8708453090299507, Silhouette: 0.7423139214515686\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fbd60c871a012bb8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
